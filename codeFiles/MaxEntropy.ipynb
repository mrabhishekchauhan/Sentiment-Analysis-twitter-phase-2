{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4fdd448ec005>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mtrain_csv_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTRAIN_PROCESSED_FILE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mtest_csv_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTEST_PROCESSED_FILE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_csv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misTrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mtraining_set_formatted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_to_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-4fdd448ec005>\u001b[0m in \u001b[0;36mget_data_from_file\u001b[1;34m(file_name, isTrain)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misTrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mbag_of_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mUSE_BIGRAMS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                     \u001b[0mbag_of_words_bigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "\n",
    "TRAIN_PROCESSED_FILE = 'C:/Users/Abhishek/Desktop/training_data(2)SVM.csv'\n",
    "TEST_PROCESSED_FILE = 'C:/Users/Abhishek/Desktop.testing_data.csv'\n",
    "USE_BIGRAMS = False\n",
    "TRAIN = True\n",
    "\n",
    "\n",
    "def get_data_from_file(file_name, isTrain=True):\n",
    "    data = []\n",
    "    with open(train_csv_file, 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        total = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if isTrain:\n",
    "                tag = line.split(',')[1]\n",
    "                bag_of_words = line.split(',')[2].split()\n",
    "                if USE_BIGRAMS:\n",
    "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[2].split()))\n",
    "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
    "            else :\n",
    "                tag = '5'\n",
    "                bag_of_words = line.split(',')[1].split()\n",
    "                if USE_BIGRAMS:\n",
    "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[1].split()))\n",
    "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
    "            data.append((bag_of_words, tag))\n",
    "    return data\n",
    "\n",
    "def split_data(tweets, validation_split=0.1):\n",
    "    index = int((1 - validation_split) * len(tweets))\n",
    "    random.shuffle(tweets)\n",
    "    return tweets[:index], tweets[index:]\n",
    "\n",
    "def list_to_dict(words_list):\n",
    "    return dict([(word, True) for word in words_list])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train = True\n",
    "    np.random.seed(1337)\n",
    "    train_csv_file = TRAIN_PROCESSED_FILE\n",
    "    test_csv_file = TEST_PROCESSED_FILE\n",
    "    train_data = get_data_from_file(train_csv_file, isTrain=True)\n",
    "    train_set, validation_set = split_data(train_data)\n",
    "    training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in train_set]\n",
    "    validation_set_formatted = [(list_to_dict(element[0]), element[1]) for element in validation_set]\n",
    "    numIterations = 1\n",
    "    algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[1]\n",
    "    classifier = nltk.MaxentClassifier.train(training_set_formatted, algorithm, max_iter=numIterations)\n",
    "    classifier.show_most_informative_features(10)\n",
    "    count = int(0)\n",
    "    for review in validation_set_formatted:\n",
    "        label = review[1]\n",
    "        text = review[0]\n",
    "        determined_label = classifier.classify(text)\n",
    "        #print(determined_label, label)\n",
    "        if determined_label!=label:\n",
    "            count+=int(1)\n",
    "    accuracy = (len(validation_set)-count)/len(validation_set)\n",
    "    print ('Validation set accuracy:%.4f% (accuracy)')\n",
    "    f = open('maxEnt_classifier.pickle', 'wb')\n",
    "    pickle.dump(classifier, f)\n",
    "    f.close()\n",
    "    print ('\\nPredicting for test data')\n",
    "    test_data = get_data_from_file(test_csv_file, isTrain=False)\n",
    "    test_set_formatted = [(list_to_dict(element[0]), element[1]) for element in test_data]\n",
    "    tweet_id = int(0)\n",
    "    results = []\n",
    "    for review in test_set_formatted:\n",
    "        text = review[0]\n",
    "        label = classifier.classify(text)\n",
    "        results.append((str(tweet_id), label))\n",
    "        tweet_id += int(1)\n",
    "    save_results_to_csv(results, 'maxent.csv')\n",
    "    print ('\\nSaved to maxent.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
